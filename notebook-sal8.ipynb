{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":107555,"databundleVersionId":13033998,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Cloning GitHub Repo</h1>","metadata":{}},{"cell_type":"code","source":"#----------Cloning GitHub Repo------------\n!git clone https://github.com/SimoneRoma21/PKMN_VictoryBinaryClassification.git\n#!git pull","metadata":{"execution":{"iopub.status.busy":"2025-11-13T08:27:07.699057Z","iopub.execute_input":"2025-11-13T08:27:07.699409Z","iopub.status.idle":"2025-11-13T08:27:10.962814Z","shell.execute_reply.started":"2025-11-13T08:27:07.699377Z","shell.execute_reply":"2025-11-13T08:27:10.961548Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#---------Appending the right path----------\nimport sys\n\n# Make sure to replace the repository folder with the actual folder name\nrepo_path = ['/kaggle/working/PKMN_VictoryBinaryClassification/py','/kaggle/working/PKMN_VictoryBinaryClassification/data']\nif repo_path not in sys.path:\n    sys.path.append(repo_path)\nprint(sys.path)\n\n%cd /kaggle/working/PKMN_VictoryBinaryClassification/py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:28:29.465619Z","iopub.execute_input":"2025-11-13T08:28:29.466086Z","iopub.status.idle":"2025-11-13T08:28:29.474012Z","shell.execute_reply.started":"2025-11-13T08:28:29.466058Z","shell.execute_reply":"2025-11-13T08:28:29.472923Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<H1>Imports</H1>","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom dataset.dataset_construction import Feature, FeaturePipeline\nfrom dataset.csv_utilities import *\nfrom dataset.extract_utilities import *\nfrom ModelTrainer import ModelTrainer\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:28:40.344863Z","iopub.execute_input":"2025-11-13T08:28:40.346599Z","iopub.status.idle":"2025-11-13T08:28:41.993085Z","shell.execute_reply.started":"2025-11-13T08:28:40.346557Z","shell.execute_reply":"2025-11-13T08:28:41.992175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<H1>Feature Selected</H1>","metadata":{}},{"cell_type":"code","source":"selected_features = [\n        \n        #------Stats Features---------#\n        Feature.P1_FINAL_TEAM_HP, \n        Feature.P2_FINAL_TEAM_HP, \n        Feature.MEAN_SPE_LAST, \n        Feature.MEAN_HP_LAST, \n        Feature.MEAN_ATK_LAST, \n        Feature.MEAN_SPA_LAST, \n        Feature.MEAN_STATS_LAST, \n        Feature.MEAN_CRIT,\n\n        #------Ratio on Stats Features--------#\n        Feature.HP_BULK_RATIO,\n        Feature.SPE_ATK_RATIO,\n        Feature.OFF_DEF_RATIO,\n        Feature.OFF_SPAD_RATIO,\n\n        #-------Differential Features on Stats---#\n        Feature.HP_TREND_DIFF,\n        Feature.ATK_TREND_DIFF,\n        Feature.SPA_TREND_DIFF,\n        Feature.SPE_TREND_DIFF,\n    \n        #---Feature Infos During Battle----#\n        Feature.P1_ALIVE_PKMN, \n        Feature.P2_ALIVE_PKMN, \n        Feature.P1_SWITCHES_COUNT, \n        Feature.P2_SWITCHES_COUNT,\n    \n        Feature.P1_AVG_HP_WHEN_SWITCHING, \n        Feature.P2_AVG_HP_WHEN_SWITCHING, \n        Feature.P1_MAX_DEBUFF_RECEIVED,\n        Feature.P2_MAX_DEBUFF_RECEIVED,\n        Feature.P1_AVG_MOVE_POWER, \n        Feature.P2_AVG_MOVE_POWER, \n        Feature.AVG_MOVE_POWER_DIFFERENCE, \n        Feature.P1_OFFENSIVE_RATIO, \n        Feature.P2_OFFENSIVE_RATIO, \n        Feature.OFFENSIVE_RATIO_DIFFERENCE, \n        Feature.P1_MOVED_FIRST_COUNT, \n        Feature.P2_MOVED_FIRST_COUNT, \n        Feature.SPEED_ADVANTAGE_RATIO, \n\n        #----Feature Status of Pokemons----#\n        Feature.P1_FROZEN_PKMN, \n        Feature.P2_FROZEN_PKMN, \n        Feature.P1_PARALIZED_PKMN, \n        Feature.P2_PARALIZED_PKMN, \n        Feature.P1_SLEEP_PKMN, \n        Feature.P2_SLEEP_PKMN, \n        Feature.P1_POISON_PKMN, \n        Feature.P2_POISON_PKMN,  \n        Feature.P1_BURNED_PKMN, \n        Feature.P2_BURNED_PKMN, \n        \n        #----Feature Pokemon Moves----#\n        Feature.P1_PKMN_REFLECT, \n        Feature.P2_PKMN_REFLECT, \n        Feature.P1_PKMN_REST, \n        Feature.P2_PKMN_REST, \n        Feature.P1_PKMN_EXPLOSION, \n        Feature.P2_PKMN_EXPLOSION, \n        Feature.P1_PKMN_THUNDERWAVE, \n        Feature.P2_PKMN_THUNDERWAVE, \n        Feature.P1_PKMN_RECOVER, \n        Feature.P2_PKMN_RECOVER, \n        Feature.P1_PKMN_TOXIC, \n        Feature.P2_PKMN_TOXIC, \n        Feature.P1_PKMN_FIRESPIN, \n        Feature.P2_PKMN_FIRESPIN,          \n    ]\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:28:57.824424Z","iopub.execute_input":"2025-11-13T08:28:57.824767Z","iopub.status.idle":"2025-11-13T08:28:57.832735Z","shell.execute_reply.started":"2025-11-13T08:28:57.824743Z","shell.execute_reply":"2025-11-13T08:28:57.831696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Main code for training and evaluation</h1>\n","metadata":{}},{"cell_type":"markdown","source":"### First Subsmission\n\nThis submission uses GridSearch on a LogisticRegression with 0.2/0.8 ratio for test/train. CrossValidation uses 5 fold and is applied by the GridSearch itself. On the public the submission is called first_submission.csv. Locally we have 84.89 training accuracy and 85.55 evaluation accuracy. On the public we get 84.93.","metadata":{}},{"cell_type":"code","source":"def evaluate_test_set(trainer: ModelTrainer, feature_list: list, test_file_path: str):\n\n    feature_pipeline = FeaturePipeline(feature_list, cache_dir=\"../data/test_feature_cache\")\n\n    print(\"\\nLoading test data...\")\n    test_data = []\n    with open(test_file_path, 'r') as f:\n        for line in f:\n            test_data.append(json.loads(line))\n\n    # Extract features from test set\n    print(\"\\nExtracting features from test data...\")\n    test_df = feature_pipeline.extract_features(test_data, show_progress=True)\n\n    X_test = test_df.drop(['battle_id'], axis=1, errors='ignore')\n\n    # Predict on test set\n    predictions = trainer.predict(X_test)\n\n    submission = pd.DataFrame({\n        'battle_id': test_df['battle_id'],\n        'player_won': predictions\n    })\n    submission.to_csv('/kaggle/working/first_submission.csv', index=False)\n\n\nfeature_pipeline = FeaturePipeline(selected_features)\n\ntrain_file_path = '/kaggle/input/fds-pokemon-battles-prediction-2025/train.jsonl'\ntest_file_path = '/kaggle/input/fds-pokemon-battles-prediction-2025/test.jsonl'\ntrain_out_path=\"predict_csv/train_features_extracted.csv\"\n\nprint(\"Loading training data...\")\ntrain_data = []\nwith open(train_file_path, 'r') as f:\n    for line in f:\n        train_data.append(json.loads(line))\n\n# Extract the features for train_set\nprint(\"\\nExtracting features from training data...\")\ntrain_df = feature_pipeline.extract_features(train_data)\nprint(\"\\nTraining features preview:\")\nprint(train_df.head())\n\n# Save dataset in a CSV file\ntrain_df.to_csv(train_out_path, index=False)\n\n#---------------Model Training and Evaluation Code------------------------\n\n# Remove row 4877 from the train dataset\ntrain_df = train_df.drop(index=4877)\nX_train = train_df.drop(['battle_id', 'player_won'], axis=1)\ny_train = train_df['player_won']\n\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Pipeline with scaler and model\nprint(\"\\nCreating pipeline with RobustScaler and LogisticRegression...\")\npipeline = Pipeline([\n    ('scaler',RobustScaler()),\n    ('classifier', LogisticRegression(random_state=42, max_iter=2000)),\n])\n\n#Grid Search for Logistic Regression \nparam_grid={\n     'classifier__C':[0.01,0.1,1,10,100],\n     'classifier__penalty': ['l1','l2'],\n     'classifier__solver':['liblinear','saga'],\n     'classifier__max_iter':[1000,2000]\n}\n\ngrid_logreg = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring='roc_auc',\n    # scoring='accuracy',\n    n_jobs=-1,  \n    cv=5,            # 5-fold cross-validation, more on this later\n    refit=True,      # retrain the best model on the full training set\n    return_train_score=True\n)\n\n\ntrainer = ModelTrainer(grid_logreg)\ntrainer.train(X_tr, y_tr)\ntrainer.evaluate(X_val, y_val)\n\nprint(\"Best CV score:\", grid_logreg.best_score_)\nprint(\"Best params:\", grid_logreg.best_params_)\n\n\n# ------------------ Evaluate on Test Set -----------------\nevaluate_test_set(trainer, selected_features, test_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:44:21.732798Z","iopub.execute_input":"2025-11-13T08:44:21.734363Z","iopub.status.idle":"2025-11-13T08:54:59.383163Z","shell.execute_reply.started":"2025-11-13T08:44:21.734326Z","shell.execute_reply":"2025-11-13T08:54:59.381881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Second Subsmission\n\nThis submission uses GridSearch on a LogisticRegression with 0.1/0.9 ratio for test/train. CrossValidation uses 5 fold and is applied by the GridSearch itself. On the public the submission is called second_submission.csv. Locally we have 85.03 training accuracy and 85.30 evaluation accuracy. On the public we get 84.73.\n","metadata":{}},{"cell_type":"code","source":"def evaluate_test_set(trainer: ModelTrainer, feature_list: list, test_file_path: str):\n\n    feature_pipeline = FeaturePipeline(feature_list, cache_dir=\"../data/test_feature_cache\")\n\n    print(\"\\nLoading test data...\")\n    test_data = []\n    with open(test_file_path, 'r') as f:\n        for line in f:\n            test_data.append(json.loads(line))\n\n    # Extract features from test set\n    print(\"\\nExtracting features from test data...\")\n    test_df = feature_pipeline.extract_features(test_data, show_progress=True)\n\n    X_test = test_df.drop(['battle_id'], axis=1, errors='ignore')\n\n    # Predict on test set\n    predictions = trainer.predict(X_test)\n\n    submission = pd.DataFrame({\n        'battle_id': test_df['battle_id'],\n        'player_won': predictions\n    })\n    submission.to_csv('/kaggle/working/second_submission.csv', index=False)\n\n\n\nfeature_pipeline = FeaturePipeline(selected_features)\n\ntrain_file_path = '/kaggle/input/fds-pokemon-battles-prediction-2025/train.jsonl'\ntest_file_path = '/kaggle/input/fds-pokemon-battles-prediction-2025/test.jsonl'\ntrain_out_path=\"predict_csv/train_features_extracted.csv\"\n\nprint(\"Loading training data...\")\ntrain_data = []\nwith open(train_file_path, 'r') as f:\n    for line in f:\n        train_data.append(json.loads(line))\n\n# Extract the features for train_set\nprint(\"\\nExtracting features from training data...\")\ntrain_df = feature_pipeline.extract_features(train_data)\nprint(\"\\nTraining features preview:\")\nprint(train_df.head())\n\n# Save dataset in a CSV file\ntrain_df.to_csv(train_out_path, index=False)\n\n#---------------Model Training and Evaluation Code------------------------\n\n# Remove row 4877 from the train dataset\ntrain_df = train_df.drop(index=4877)\nX_train = train_df.drop(['battle_id', 'player_won'], axis=1)\ny_train = train_df['player_won']\n\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n\n# Pipeline with scaler and model\nprint(\"\\nCreating pipeline with RobustScaler and LogisticRegression...\")\npipeline = Pipeline([\n    ('scaler',RobustScaler()),\n    ('classifier', LogisticRegression(random_state=42, max_iter=2000)),\n])\n\n#Grid Search for Logistic Regression \nparam_grid={\n     'classifier__C':[0.01,0.1,1,10,100],\n     'classifier__penalty': ['l1','l2'],\n     'classifier__solver':['liblinear','saga'],\n     'classifier__max_iter':[1000,2000]\n}\n\ngrid_logreg = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring='roc_auc',\n    # scoring='accuracy',\n    n_jobs=-1,  \n    cv=5,            # 5-fold cross-validation, more on this later\n    refit=True,      # retrain the best model on the full training set\n    return_train_score=True\n)\n\n\ntrainer = ModelTrainer(grid_logreg)\ntrainer.train(X_tr, y_tr)\ntrainer.evaluate(X_val, y_val)\n\nprint(\"Best CV score:\", grid_logreg.best_score_)\nprint(\"Best params:\", grid_logreg.best_params_)\n\n\n# ------------------ Evaluate on Test Set -----------------\n\nevaluate_test_set(trainer, selected_features, test_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:07:48.882527Z","iopub.execute_input":"2025-11-13T09:07:48.882905Z","iopub.status.idle":"2025-11-13T09:19:40.248871Z","shell.execute_reply.started":"2025-11-13T09:07:48.882878Z","shell.execute_reply":"2025-11-13T09:19:40.247645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Third Subsmission\nThis submission uses ModelStacking where the base estimators are LogisticRegression, SGDClassifier, XGBClassifier and RandomForest. We used 2 linear models and 2 non-linear models to combine the advantages of both types. Each of the base estimators has been fine-tuned separately using GridSearchCV on the same training data before being included in the stacking ensemble. The meta model is LogisticRegression. Split ratio is 0.2/0.8 for test/train. CrossValidation uses 5 fold and is applied by the GridSearch itself. On the public the submission is called third_submission.csv. Locally we have xxxx training accuracy and xxxx evaluation accuracy. On the public we get 84.20.","metadata":{}},{"cell_type":"code","source":"def evaluate_test_set(trainer: ModelTrainer, feature_list: list, test_file_path: str):\n\n    feature_pipeline = FeaturePipeline(feature_list, cache_dir=\"../data/test_feature_cache\")\n\n    print(\"\\nLoading test data...\")\n    test_data = []\n    with open(test_file_path, 'r') as f:\n        for line in f:\n            test_data.append(json.loads(line))\n\n    # Extract features from test set\n    print(\"\\nExtracting features from test data...\")\n    test_df = feature_pipeline.extract_features(test_data, show_progress=True)\n\n    X_test = test_df.drop(['battle_id'], axis=1, errors='ignore')\n\n    # Predict on test set\n    predictions = trainer.predict(X_test)\n\n    submission = pd.DataFrame({\n        'battle_id': test_df['battle_id'],\n        'player_won': predictions\n    })\n    submission.to_csv('/kaggle/working/third_submission.csv', index=False)\n\nfeature_pipeline = FeaturePipeline(selected_features)\n\ntrain_file_path = '/kaggle/input/fds-pokemon-battles-prediction-2025/train.jsonl'\ntest_file_path = '/kaggle/input/fds-pokemon-battles-prediction-2025/test.jsonl'\ntrain_out_path=\"predict_csv/train_features_extracted.csv\"\n\nprint(\"Loading training data...\")\ntrain_data = []\nwith open(train_file_path, 'r') as f:\n    for line in f:\n        train_data.append(json.loads(line))\n\n# Extract the features for train_set\nprint(\"\\nExtracting features from training data...\")\ntrain_df = feature_pipeline.extract_features(train_data)\nprint(\"\\nTraining features preview:\")\nprint(train_df.head())\n\n# Save dataset in a CSV file\ntrain_df.to_csv(train_out_path, index=False)\n\n#---------------Model Training and Evaluation Code------------------------\n\n# Remove row 4877 from the train dataset\ntrain_df = train_df.drop(index=4877)\nX_train = train_df.drop(['battle_id', 'player_won'], axis=1)\ny_train = train_df['player_won']\n\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Pipelines: Scaling only the linear models\npipe_lr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('lr', LogisticRegression(max_iter=1000,C=1,penalty='l1',solver='liblinear',random_state=42))\n])\n\npipe_sgd = Pipeline([\n    ('scaler', RobustScaler()),\n    ('sgd', SGDClassifier(\n        loss='modified_huber',\n        penalty='l1',\n        alpha=0.001,\n        learning_rate='adaptive',\n        eta0=0.01,\n        max_iter=1000,\n        tol=0.0001,\n        random_state=42\n    ))\n])\n\n# Tree and Boosting\nxgb = XGBClassifier(\n    eval_metric='logloss',\n    random_state=42,\n    colsample_bytree=0.8,\n    gamma=0.3,\n    learning_rate=0.05,\n    max_depth=3,\n    min_child_weight=5,\n    n_estimators=400,\n    reg_alpha=0,\n    reg_lambda=2,\n    subsample=0.8\n    )\n\nrf = RandomForestClassifier(\n    random_state=42, bootstrap=True,\n    max_depth=12, max_features='sqrt',\n    min_samples_leaf=3, min_samples_split=5, n_estimators=300\n    )\n\n# Base estimators\nbase_estimators = [\n    ('lr', pipe_lr),\n    ('xgb', xgb),\n    ('rf', rf),\n    ('sgd', pipe_sgd)\n]\n\n# Meta model with gridSearchCV for the hyperparameters\nmeta = LogisticRegression(random_state=42)\n\nstack = StackingClassifier(\n    estimators=base_estimators,\n    final_estimator=meta,\n    cv=5,\n    stack_method='predict_proba',   \n    n_jobs=-1,\n    passthrough=False\n)\nparam_grid = {\n'final_estimator__C': [0.01, 0.1, 1, 10, 100],\n'final_estimator__penalty': ['l1', 'l2'],\n'final_estimator__solver': ['liblinear', 'saga'],\n'final_estimator__max_iter': [1000, 2000]\n}\n\ngs = GridSearchCV(stack, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n\ntrainer = ModelTrainer(gs)\ntrainer.train(X_tr,y_tr)\ntrainer.evaluate(X_val,y_val)\n\nprint(\"Best CV score:\", gs.best_score_)\nprint(\"Best params:\", gs.best_params_)\n\n# ------------------ Evaluate on Test Set -----------------\n\nevaluate_test_set(trainer, selected_features, test_file_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}